{
    "ec2": {
        "benchmark-0": {
            "result": {
                "error": "Some VPCs do not have flow logging enabled",
                "name": "3.7 Ensure VPC flow logging is enabled in all VPCs",
                "output": "VPCs without flow logging: ap-south-1: vpc-09123e47ad76e161e eu-north-1: vpc-070aa1b283a44e82a eu-west-3: vpc-044b9f1340455565b eu-west-2: vpc-082903522cbde0839 eu-west-1: vpc-0bdb707bf45139478 ap-northeast-3: vpc-0dafdaa297c988848 ap-northeast-2: vpc-0462011c310215745 ap-northeast-1: vpc-03ff2360b491a0570 ca-central-1: vpc-001b948197256078b sa-east-1: vpc-03a4248878103e2f0 ap-southeast-1: vpc-0f8144ea4375c89ad ap-southeast-2: vpc-022473b6d0dc6a5b4 eu-central-1: vpc-018dfb4c5ca5a6c20 us-east-1: vpc-02ee35a676ecb11b1 us-east-2: vpc-08f3552302001ad0f us-west-1: vpc-0b25e9c5210848e86 us-west-2: vpc-0cb6385e766c84f3f",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"3.7 Ensure VPC flow logging is enabled in all VPCs\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Function to check VPC flow logs in a region\ncheck_vpc_flow_logs() {\n    local region=$1\n    local non_compliant_vpcs=()\n\n    # List all VPCs in the region\n    vpc_ids=$(aws ec2 describe-vpcs --region \"$region\" --query 'Vpcs[*].VpcId' --output text)\n\n    for vpc_id in $vpc_ids; do\n        # Check if flow logs exist for this VPC\n        flow_logs=$(aws ec2 describe-flow-logs --region \"$region\" --filter \"Name=resource-id,Values=$vpc_id\" --query 'FlowLogs[?FlowLogStatus==`ACTIVE`]' --output text)\n\n        if [ -z \"$flow_logs\" ]; then\n            non_compliant_vpcs+=(\"$vpc_id\")\n        fi\n    done\n\n    echo \"${non_compliant_vpcs[@]}\"\n}\n\n# Get all regions\nregions=$(aws ec2 describe-regions --query 'Regions[*].RegionName' --output text)\n\nnon_compliant_regions=()\nfor region in $regions; do\n    region_non_compliant_vpcs=$(check_vpc_flow_logs \"$region\")\n    \n    if [ -n \"$region_non_compliant_vpcs\" ]; then\n        non_compliant_regions+=(\"$region: ${region_non_compliant_vpcs}\")\n    fi\ndone\n\nif [ ${#non_compliant_regions[@]} -eq 0 ]; then\n    STATUS=\"success\"\n    OUTPUT=\"All VPCs have flow logging enabled.\"\n    ERROR=\"\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"VPCs without flow logging: ${non_compliant_regions[*]}\"\n    ERROR=\"Some VPCs do not have flow logging enabled\"\nfi\n\necho '{\"status\": \"'$STATUS'\", \"error\": \"'\"$ERROR\"'\", \"name\": \"3.7 Ensure VPC flow logging is enabled in all VPCs\", \"output\": \"'\"$OUTPUT\"'\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\ncat \"$OUTPUT_FILE\"",
            "text": "Profile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic\ngoing to and from network interfaces in your VPC. After you've created a flow log, you\ncan view and retrieve its data in Amazon CloudWatch Logs. It is recommended that\nVPC Flow Logs be enabled for packet \"Rejects\" for VPCs.\n\nRationale:\n\nVPC Flow Logs provide visibility into network traffic that traverses the VPC and can be\nused to detect anomalous traffic or gain insights during security workflows.\n\nImpact:\n\nBy default, CloudWatch Logs will store logs indefinitely unless a specific retention\nperiod is defined for the log group. When choosing the number of days to retain, keep in\nmind that the average time it takes for an organization to realize they have been\nbreached is 210 days (at the time of this writing). Since additional time is required to\nresearch a breach, a minimum retention policy of 365 days allows for detection and\ninvestigation. You may also wish to archive the logs to a cheaper storage service rather\nthan simply deleting them. See the following AWS resource to manage CloudWatch\nLogs retention periods:\n\n1.  https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/Settin\n\ngLogRetention.html\n\nAudit:\n\nPerform the following to determine if VPC Flow logs are enabled:\nFrom Console:\n\n1.  Sign into the management console.\n2.  Select Services, then select VPC.\n3.  In the left navigation pane, select Your VPCs.\n4.  Select a VPC.\n5.  In the right pane, select the Flow Logs tab.\n6.  Ensure a Log Flow exists that has Active in the Status column.\n\nFrom Command Line:\n\n1.  Run the describe-vpcs command (OSX/Linux/UNIX) to list the VPC networks\n\navailable in the current AWS region:\n\nPage 135\n\nInternal Only - General\n\n\n```bash\naws ec2 describe-vpcs --region <region> --query Vpcs[].VpcId\n```\n\n\n2.  The command output returns the VpcId of VPCs available in the selected region.\n3.  Run the describe-flow-logs command (OSX/Linux/UNIX) using the VPC ID to\ndetermine if the selected virtual network has the Flow Logs feature enabled:\n\n\n```bash\naws ec2 describe-flow-logs --filter \"Name=resource-id,Values=<vpc-id>\"\n```\n\n\n\u2022\n\nIf there are no Flow Logs created for the selected VPC, the command output will\nreturn an empty list [].\n\n4.  Repeat step 3 for other VPCs in the same region.\n5.  Change the region by updating --region, and repeat steps 1-4 for each region.\n\nRemediation:\n\nPerform the following to enable VPC Flow Logs:\nFrom Console:\n\n1.  Sign into the management console.\n2.  Select Services, then select VPC.\n3.  In the left navigation pane, select Your VPCs.\n4.  Select a VPC.\n5.  In the right pane, select the Flow Logs tab.\n6.  If no Flow Log exists, click Create Flow Log.\n7.  For Filter, select Reject.\n8.  Enter a Role and Destination Log Group.\n9.  Click Create Log Flow.\n10. Click on CloudWatch Logs Group.\n\nNote: Setting the filter to \"Reject\" will dramatically reduce the accumulation of logging\ndata for this recommendation and provide sufficient information for the purposes of\nbreach detection, research, and remediation. However, during periods of least privilege\nsecurity group engineering, setting the filter to \"All\" can be very helpful in discovering\nexisting traffic flows required for the proper operation of an already running\nenvironment.\nFrom Command Line:\n\n1.  Create a policy document, name it role_policy_document.json, and paste the\n\nfollowing content:\n\nInternal Only - General\n\nPage 136\n\n\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"test\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"ec2.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n\n2.  Create another policy document, name it iam_policy.json, and paste the\n\nfollowing content:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\":[\n\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:DescribeLogGroups\",\n\"logs:DescribeLogStreams\",\n\"logs:PutLogEvents\",\n\"logs:GetLogEvents\",\n\"logs:FilterLogEvents\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n\n3.  Run the following command to create an IAM role:\n\n\n```bash\naws iam create-role --role-name <aws-support-iam-role> --assume-role-policy-\n```\n\ndocument file://<file-path>role_policy_document.json\n\n4.  Run the following command to create an IAM policy:\n\n\n```bash\naws iam create-policy --policy-name <iam-policy-name> --policy-document\n```\n\nfile://<file-path>iam-policy.json\n\nInternal Only - General\n\nPage 137\n\n\n\n\n\n5.  Run the attach-group-policy command, using the IAM policy ARN returned\n\nfrom the previous step to attach the policy to the IAM role:\n\n\n```bash\naws iam attach-group-policy --policy-arn arn:aws:iam::<aws-account-\n```\n\nid>:policy/<iam-policy-name> --group-name <group-name>\n\n\u2022\n\nIf the command succeeds, no output is returned.\n\n6.  Run the describe-vpcs command to get a list of VPCs in the selected region:\n\n\n```bash\naws ec2 describe-vpcs --region <region>\n```\n\n\n\u2022  The command output should return a list of VPCs in the selected region.\n\n7.  Run the create-flow-logs command to create a flow log for a VPC:\n\n\n```bash\naws ec2 create-flow-logs --resource-type VPC --resource-ids <vpc-id> --\n```\n\ntraffic-type REJECT --log-group-name <log-group-name> --deliver-logs-\npermission-arn <iam-role-arn>\n\n8.  Repeat step 7 for other VPCs in the selected region.\n9.  Change the region by updating --region, and repeat the remediation procedure\n\nfor each region.\n\nReferences:\n\n1.  CCE-79202-8\n2.  https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv8",
            "title": "3.7_Ensure_VPC_flow_logging_is_enabled_in_all_VPCs_(Automated).md"
        },
        "benchmark-1": {
            "result": {
                "error": "",
                "name": "Ensure S3 Bucket Policy is set to deny HTTP requests",
                "output": "Non-compliant buckets: testbucketsiol",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\n# Check AWS CLI configuration\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"Ensure S3 Bucket Policy is set to deny HTTP requests\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Function to check bucket policy for secure transport\ncheck_bucket_secure_transport() {\n    local bucket_name=\"$1\"\n    local policy\n    \n    # Retrieve bucket policy\n    if policy=$(aws s3api get-bucket-policy --bucket \"$bucket_name\" --query Policy --output text 2>/dev/null); then\n        # Check for secure transport denial conditions\n        if echo \"$policy\" | grep -q '\"aws:SecureTransport\": \"false\"' && \n           echo \"$policy\" | grep -q '\"Effect\": \"Deny\"' && \n           echo \"$policy\" | grep -q '\"Principal\": \"*\"'; then\n            return 0  # Compliant policy found\n        fi\n        \n        # Check for TLS version condition\n        if echo \"$policy\" | grep -q '\"s3:TlsVersion\": \"1.2\"' && \n           echo \"$policy\" | grep -q '\"Effect\": \"Deny\"' && \n           echo \"$policy\" | grep -q '\"Principal\": \"*\"'; then\n            return 0  # Compliant policy found\n        fi\n    fi\n    \n    return 1  # Non-compliant or no policy\n}\n\n# Get list of S3 buckets\nbuckets=$(aws s3 ls | awk '{print $3}')\n\nnon_compliant_buckets=()\n\n# Check each bucket\nfor bucket in $buckets; do\n    if ! check_bucket_secure_transport \"$bucket\"; then\n        non_compliant_buckets+=(\"$bucket\")\n    fi\ndone\n\n# Determine test result\nif [ ${#non_compliant_buckets[@]} -eq 0 ]; then\n    STATUS=\"success\"\n    OUTPUT=\"All S3 buckets have secure transport policies configured.\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"Non-compliant buckets: ${non_compliant_buckets[*]}\"\nfi\n\n# Generate output JSON\necho '{\n    \"status\": \"'$STATUS'\", \n    \"error\": \"\", \n    \"name\": \"Ensure S3 Bucket Policy is set to deny HTTP requests\", \n    \"output\": \"'$OUTPUT'\", \n    \"type\": \"automated\"\n}' > \"$OUTPUT_FILE\"\n\ncat \"$OUTPUT_FILE\"",
            "text": "(Automated)\n\nProfile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nAt the Amazon S3 bucket level, you can configure permissions through a bucket policy,\nmaking the objects accessible only through HTTPS.\n\nRationale:\n\nBy default, Amazon S3 allows both HTTP and HTTPS requests. To ensure that access\nto Amazon S3 objects is only permitted through HTTPS, you must explicitly deny HTTP\nrequests. Bucket policies that allow HTTPS requests without explicitly denying HTTP\nrequests will not comply with this recommendation.\n\nAudit:\n\nTo allow access to HTTPS, you can use a bucket policy with the effect allow and a\ncondition that checks for the key \"aws:SecureTransport\": \"true\". This means that\nHTTPS requests are allowed, but it does not deny HTTP requests. To explicitly deny\nHTTP access, ensure that there is also a bucket policy with the effect deny that contains\nthe key \"aws:SecureTransport\": \"false\". You may also require TLS by setting a\npolicy to deny any version lower than the one you wish to require, using the condition\nNumericLessThan and the key \"s3:TlsVersion\": \"1.2\".\nFrom Console:\n\n1.  Login to the AWS Management Console and open the Amazon S3 console using\n\nhttps://console.aws.amazon.com/s3/.\n2.  Select the check box next to the Bucket.\n3.  Click on 'Permissions', then click on Bucket Policy.\n4.  Ensure that a policy is listed that matches either:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\nInternal Only - General\n\nPage 76\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n<optional> and <bucket_name> will be specific to your account, and TLS version will\nbe site/policy specific to your organisation.\n\n5.  Repeat for all the buckets in your AWS account.\n\nFrom Command Line:\n\n1.  List all of the S3 Buckets\n\n\n```bash\naws s3 ls\n```\n\n\n2.  Using the list of buckets, run this command on each of them:\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> | grep aws:SecureTransport\n```\n\n\nor\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> | grep s3:TlsVersion\n```\n\n\nNOTE : If an error is thrown by the CLI, it means no policy has been configured for the\nspecified S3 bucket, and that by default it is allowing both HTTP and HTTPS requests.\n\n3.  Confirm that aws:SecureTransport is set to false (such as\n\naws:SecureTransport:false) or that s3:TlsVersion has a site-specific value.\n\n4.  Confirm that the policy line has Effect set to Deny 'Effect:Deny'\n\nRemediation:\n\nFrom Console:\n\nInternal Only - General\n\nPage 77\n\n\n\n\n1.  Login to the AWS Management Console and open the Amazon S3 console using\n\nhttps://console.aws.amazon.com/s3/.\n2.  Select the check box next to the Bucket.\n3.  Click on 'Permissions'.\n4.  Click 'Bucket Policy'.\n5.  Add either of the following to the existing policy, filling in the required information:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n6.  Save\n7.  Repeat for all the buckets in your AWS account that contain sensitive data.\n\nFrom Console\nUsing AWS Policy Generator:\n\n1.  Repeat steps 1-4 above.\n2.  Click on Policy Generator at the bottom of the Bucket Policy Editor.\n3.  Select Policy Type S3 Bucket Policy.\n4.  Add Statements:\n\n\u2022\n\nEffect = Deny\n\nInternal Only - General\n\nPage 78\n\n\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nPrincipal = *\n\nAWS Service = Amazon S3\n\nActions = *\n\nAmazon Resource Name = <ARN of the S3 Bucket>\n\n5.  Generate Policy.\n6.  Copy the text and add it to the Bucket Policy.\n\nFrom Command Line:\n\n1.  Export the bucket policy to a json file:\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> --query Policy --output\n```\n\ntext > policy.json\n\n2.  Modify the policy.json file by adding either of the following:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\nInternal Only - General\n\nPage 79\n\n\n\n\n\n\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n3.  Apply this modified policy back to the S3 bucket:\n\n\n```bash\naws s3api put-bucket-policy --bucket <bucket_name> --policy\n```\n\nfile://policy.json\n\nDefault Value:\n\nBoth HTTP and HTTPS requests are allowed.\n\nReferences:\n\n1.  https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-\n\nconfig-rule/\n\n2.  https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-\n\ndefense-in-depth-to-help-secure-your-amazon-s3-data/\n\n3.  https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-\n\nbucket-policy.html\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv7",
            "title": "2.1.1_Ensure_S3_Bucket_Policy_is_set_to_deny_HTTP_requests.md"
        },
        "benchmark-2": {
            "result": {
                "error": "",
                "name": "Ensure S3 Bucket Policy is set to deny HTTP requests",
                "output": "Non-compliant buckets: testbucketsiol",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"Ensure routing tables for VPC peering are least access\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"Ensure routing tables for VPC peering are least access\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Get all VPC peering connections\nPEERING_CONNECTIONS=$(aws ec2 describe-vpc-peering-connections --query 'VpcPeeringConnections[*].VpcPeeringConnectionId' --output text)\n\nCOMPLIANCE_ISSUES=()\n\nfor PEERING_CONNECTION in $PEERING_CONNECTIONS; do\n    # Get VPCs involved in the peering connection\n    VPC_INFO=$(aws ec2 describe-vpc-peering-connections --vpc-peering-connection-ids \"$PEERING_CONNECTION\" --query 'VpcPeeringConnections[0].{RequesterVpcId:RequesterVpcId,AccepterVpcId:AccepterVpcId}' --output json)\n    REQUESTER_VPC=$(echo \"$VPC_INFO\" | grep -o '\"RequesterVpcId\": *\"[^\"]*' | cut -d'\"' -f4)\n    ACCEPTER_VPC=$(echo \"$VPC_INFO\" | grep -o '\"AccepterVpcId\": *\"[^\"]*' | cut -d'\"' -f4)\n\n    # Check route tables for both VPCs\n    for VPC in \"$REQUESTER_VPC\" \"$ACCEPTER_VPC\"; do\n        ROUTE_TABLES=$(aws ec2 describe-route-tables --filters \"Name=vpc-id,Values=$VPC\" --query 'RouteTables[*].RouteTableId' --output text)\n        \n        for ROUTE_TABLE in $ROUTE_TABLES; do\n            # Check routes for peering connections\n            PEERING_ROUTES=$(aws ec2 describe-route-tables --route-table-ids \"$ROUTE_TABLE\" --query \"RouteTables[*].Routes[?GatewayId=='$PEERING_CONNECTION']\" --output json)\n            \n            if [ -n \"$PEERING_ROUTES\" ] && [ \"$PEERING_ROUTES\" != \"[]\" ]; then\n                # Analyze route specificity\n                ROUTES=$(aws ec2 describe-route-tables --route-table-ids \"$ROUTE_TABLE\" --query \"RouteTables[*].Routes[?GatewayId=='$PEERING_CONNECTION'].DestinationCidrBlock\" --output text)\n                \n                for ROUTE in $ROUTES; do\n                    # Check if route is too broad (e.g., 0.0.0.0/0 or large CIDR blocks)\n                    if [[ \"$ROUTE\" == \"0.0.0.0/0\" ]] || [[ \"$ROUTE\" =~ /([0-9]|1[0-5])$ ]]; then\n                        COMPLIANCE_ISSUES+=(\"Route table $ROUTE_TABLE has overly broad peering route to $ROUTE\")\n                    fi\n                done\n            fi\n        done\n    done\ndone\n\n# Determine compliance status\nif [ ${#COMPLIANCE_ISSUES[@]} -eq 0 ]; then\n    STATUS=\"success\"\n    OUTPUT=\"All VPC peering route tables appear to be following least access principle.\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=$(printf '%s\\n' \"${COMPLIANCE_ISSUES[@]}\")\nfi\n\n# Generate output JSON\necho '{\"status\": \"'$STATUS'\", \"error\": \"\", \"name\": \"Ensure routing tables for VPC peering are least access\", \"output\": \"'\"$OUTPUT\"'\", \"type\": \"automated\"}' > \"$",
            "text": "(Manual)\n\nProfile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nOnce a VPC peering connection is established, routing tables must be updated to\nenable any connections between the peered VPCs. These routes can be as specific as\ndesired, even allowing for the peering of a VPC to only a single host on the other side of\nthe connection.\n\nRationale:\n\nBeing highly selective in peering routing tables is a very effective way to minimize the\nimpact of a breach, as resources outside of these routes are inaccessible to the peered\nVPC.\n\nAudit:\n\nReview the routing tables of peered VPCs to determine whether they route all subnets\nof each VPC and whether this is necessary to accomplish the intended purposes of\npeering the VPCs.\nFrom Command Line:\n\n1.  List all the route tables from a VPC and check if the \"GatewayId\" is pointing to a\n\n<peering-connection-id> (e.g., pcx-1a2b3c4d) and if the\n\"DestinationCidrBlock\" is as specific as desired:\n\n\n```bash\naws ec2 describe-route-tables --filter \"Name=vpc-id,Values=<vpc-id>\" --query\n```\n\n\"RouteTables[*].{RouteTableId:RouteTableId, VpcId:VpcId, Routes:Routes,\nAssociatedSubnets:Associations[*].SubnetId}\"\n\nRemediation:\n\nRemove and add route table entries to ensure that the least number of subnets or hosts\nrequired to accomplish the purpose of peering are routable.\nFrom Command Line:\n\n1.  For each <route-table-id> that contains routes that are non-compliant with\n\nyour routing policy (granting more access than desired), delete the non-compliant\nroute:\n\n\n```bash\naws ec2 delete-route --route-table-id <route-table-id> --destination-cidr-\n```\n\nblock <non-compliant-destination-cidr>\n\nInternal Only - General\n\nPage 232\n\n\n2.  Create a new compliant route:\n\n\n```bash\naws ec2 create-route --route-table-id <route-table-id> --destination-cidr-\n```\n\nblock <compliant-destination-cidr> --vpc-peering-connection-id <peering-\nconnection-id>\n\nReferences:\n\n1.  https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/peering-\n\nconfigurations-partial-access.html\n\n2.  https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/create-\n\nvpc-peering-connection.html\n\nAdditional Information:\n\nIf an organization has an AWS Transit Gateway implemented in its VPC architecture, it\nshould look to apply the recommendation above for a \"least access\" routing architecture\nat the AWS Transit Gateway level, in combination with what must be implemented at\nthe standard VPC route table. More specifically, to route traffic between two or more\nVPCs via a Transit Gateway, VPCs must have an attachment to a Transit Gateway\nroute table as well as a route. Therefore, to avoid routing traffic between VPCs, an\nattachment to the Transit Gateway route table should only be added where there is an\nintention to route traffic between the VPCs. As Transit Gateways are capable of hosting\nmultiple route tables, it is possible to group VPCs by attaching them to a common route\ntable.\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv7",
            "title": "5.6_Ensure_routing_tables_for_VPC_peering_are_\"least_access\".md"
        },
        "benchmark-3": {
            "result": {
                "error": "No multi-region CloudTrail trail found",
                "name": "4.13 Ensure route table changes are monitored",
                "output": "",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\n# Check AWS CLI configuration\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Check if AWS CLI is configured\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Find multi-region CloudTrail trail\nMULTI_REGION_TRAIL=$(aws cloudtrail describe-trails --query 'trailList[?IsMultiRegionTrail==`true`].Name' --output text)\n\nif [ -z \"$MULTI_REGION_TRAIL\" ]; then\n    echo '{\"status\": \"fail\", \"error\": \"No multi-region CloudTrail trail found\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Check trail logging status\nTRAIL_STATUS=$(aws cloudtrail get-trail-status --name \"$MULTI_REGION_TRAIL\" --query 'IsLogging' --output text)\n\nif [ \"$TRAIL_STATUS\" != \"true\" ]; then\n    echo '{\"status\": \"fail\", \"error\": \"Multi-region trail is not logging\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Get log group name\nLOG_GROUP=$(aws cloudtrail describe-trails --query \"trailList[?Name=='$MULTI_REGION_TRAIL'].CloudWatchLogsLogGroupArn\" --output text | cut -d: -f7)\n\n# Check metric filters for route table changes\nROUTE_TABLE_METRIC_FILTER=$(aws logs describe-metric-filters \\\n    --log-group-name \"$LOG_GROUP\" \\\n    --query 'metricFilters[?filterPattern contains \"CreateRoute\" && filterPattern contains \"CreateRouteTable\" && filterPattern contains \"ReplaceRoute\" && filterPattern contains \"ReplaceRouteTableAssociation\" && filterPattern contains \"DeleteRouteTable\" && filterPattern contains \"DeleteRoute\" && filterPattern contains \"DisassociateRouteTable\"]' \\\n    --output text)\n\nif [ -z \"$ROUTE_TABLE_METRIC_FILTER\" ]; then\n    echo '{\"status\": \"fail\", \"error\": \"No metric filter for route table changes found\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Check CloudWatch alarms for the metric\nROUTE_TABLE_ALARM=$(aws cloudwatch describe-alarms --query 'MetricAlarms[?contains(Namespace, \"CISBenchmark\") && contains(MetricName, \"RouteTableChanges\")]' --output text)\n\nif [ -z \"$ROUTE_TABLE_ALARM\" ]; then\n    echo '{\"status\": \"fail\", \"error\": \"No CloudWatch alarm for route table changes found\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Succeeded\necho '{\"status\": \"success\", \"error\": \"\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"Route table changes monitoring is configured\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n\ncat \"$OUTPUT_FILE\"",
            "text": "Profile Applicability:\n\n\u2022  Level 1\n\nDescription:\n\nReal-time monitoring of API calls can be achieved by directing CloudTrail Logs to\nCloudWatch Logs or an external Security Information and Event Management (SIEM)\nenvironment, and establishing corresponding metric filters and alarms.\n\nRouting tables are used to route network traffic between subnets and to network\ngateways.\n\nIt is recommended that a metric filter and alarm be established for changes to route\ntables.\n\nRationale:\n\nCloudWatch is an AWS native service that allows you to observe and monitor resources\nand applications. CloudTrail logs can also be sent to an external Security Information\nand Event Management (SIEM) environment for monitoring and alerting.\n\nMonitoring changes to route tables will help ensure that all VPC traffic flows through the\nexpected path and prevent any accidental or intentional modifications that may lead to\nuncontrolled network traffic. An alarm should be triggered every time an AWS API call is\nperformed to create, replace, delete, or disassociate a route table.\n\nAudit:\n\nIf you are using CloudTrail trails and CloudWatch, perform the following to ensure that\nthere is at least one active multi-region CloudTrail trail with the prescribed metric filters\nand alarms configured:\n\n1.  Identify the log group name that is configured for use with the active multi-region\n\nCloudTrail trail:\n\n\u2022  List all CloudTrail trails: aws cloudtrail describe-trails\n\u2022\n\nIdentify multi-region CloudTrail trails: Trails with \"IsMultiRegionTrail\"\nset to true\n\n\u2022  Note the value associated with \"Name\":<trail-name>\n\u2022  Note the <trail-log-group-name> within the value associated with\n\n\"CloudWatchLogsLogGroupArn\"\n\no  Example: arn:aws:logs:<region>:<account-id>:log-\n\ngroup:<trail-log-group-name>:*\n\n\u2022  Ensure the identified multi-region CloudTrail trail is active:\n\no  aws cloudtrail get-trail-status --name <trail-name>\n\n\u25aa  Ensure IsLogging is set to TRUE\n\nPage 200\n\nInternal Only - General\n\n\u2022  Ensure the identified multi-region CloudTrail trail captures all management\n\nevents:\n\no  aws cloudtrail get-event-selectors --trail-name <trail-name>\n\u25aa  Ensure there is at least one event selector for a trail with\n\nIncludeManagementEvents set to true and ReadWriteType set to\nAll\n\n2.  Get a list of all associated metric filters for the <trail-log-group-name>\n\ncaptured in step 1:\n\n\n```bash\naws logs describe-metric-filters --log-group-name <trail-log-group-\n```\n\nname>\n\n3.  Ensure the output from the above command contains the following:\n\n\"filterPattern\": \"{($.eventSource = ec2.amazonaws.com) && ($.eventName\n= CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName =\nReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) ||\n($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) ||\n($.eventName = DisassociateRouteTable) }\"\n\n4.  Note the <route-table-changes-metric> value associated with the\n\nfilterPattern from step 3.\n\n5.  Get a list of CloudWatch alarms, and filter on the <route-table-changes-\n\nmetric> captured in step 4:\n\n\n```bash\naws cloudwatch describe-alarms --query\n```\n\n'MetricAlarms[?MetricName==<route-table-changes-metric>]'\n\n6.  Note the AlarmActions value; this will provide the SNS topic ARN value.\n7.  Ensure there is at least one active subscriber to the SNS topic:\n\n\n```bash\naws sns list-subscriptions-by-topic --topic-arn <sns-topic-arn>\n```\n\n\n\u2022  At least one subscription should have \"SubscriptionArn\" with a valid AWS ARN.\no  Example of valid \"SubscriptionArn\": arn:aws:sns:<region>:<account-\n\nid>:<sns-topic-name>:<subscription-id>\n\nRemediation:\n\nIf you are using CloudTrail trails and CloudWatch, perform the following steps to set up\nthe metric filter, alarm, SNS topic, and subscription:\n\n1.  Create a metric filter based on the provided filter pattern that checks for route\n\ntable changes and uses the <trail-log-group-name> taken from audit step 1:\n\nInternal Only - General\n\nPage 201\n\n\n\n```bash\naws logs put-metric-filter --log-group-name <trail-log-group-name> --\n```\n\nfilter-pattern '{ ($.eventName = CreateRoute) || ($.eventName =\nCreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName =\nReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) ||\n($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable)\n}'\n\nNote: You can choose your own metricName and metricNamespace strings.\nUsing the same metricNamespace for all Foundations Benchmark metrics will\ngroup them together.\n\n2.  Create an SNS topic that the alarm will notify:\n\n\n```bash\naws sns create-topic --name <sns-topic-name>\n```\n\n\nNote: You can execute this command once and then reuse the same topic for all\nmonitoring alarms.\nNote: Capture the TopicArn that is displayed when creating the SNS topic in\nstep 2.\n\n3.  Create an SNS subscription for the topic created in step 2:\n\n\n```bash\naws sns subscribe --topic-arn <sns-topic-arn> --protocol <sns-protocol>\n```\n\n--notification-endpoint <sns-subscription-endpoints>\n\nNote: You can execute this command once and then reuse the same\nsubscription for all monitoring alarms.\n\n4.  Create an alarm that is associated with the CloudWatch Logs metric filter created\n\nin step 1 and the SNS topic created in step 2:\n\n\n```bash\naws cloudwatch put-metric-alarm --alarm-name <route-table-changes-\n```\n\nalarm> --metric-name <route-table-changes-metric> --statistic Sum --\nperiod 300 --threshold 1 --comparison-operator\nGreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace\n'CISBenchmark' --alarm-actions <sns-topic-arn>\n\nReferences:\n\n1.  CCE-79198-8\n2.  https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-\n\nlog-files-from-multiple-regions.html\n\n3.  https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-\n\nfor-cloudtrail.html\n\n4.  https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html\n\nInternal Only - General\n\nPage 202\n\nAdditional Information:\n\nConfiguring a log metric filter and alarm on a multi-region (global) CloudTrail trail:\n\n\u2022  ensures that activities from all regions (both used and unused) are monitored\n\u2022  ensures that activities on all supported global services are monitored\n\u2022  ensures that all management events across all regions are monitored\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv8\n\nv7\n\nv7\n\nv7",
            "title": "4.13_Ensure_route_table_changes_are_monitored_(Manual).md"
        },
        "benchmark-4": {
            "result": {
                "error": "",
                "name": "Protect Information through Access Control Lists",
                "output": "All S3 buckets have appropriate access controls",
                "status": "success",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"Protect Information through Access Control Lists\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"Protect Information through Access Control Lists\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Check S3 bucket ACLs and policies\nS3_BUCKETS=$(aws s3 ls)\nINSECURE_BUCKETS=\"\"\n\nwhile read -r bucket; do\n    BUCKET_NAME=$(echo \"$bucket\" | awk '{print $3}')\n    \n    # Check bucket ACL\n    ACL=$(aws s3api get-bucket-acl --bucket \"$BUCKET_NAME\")\n    if echo \"$ACL\" | grep -q \"AllUsers\\|AuthenticatedUsers\"; then\n        INSECURE_BUCKETS+=\"$BUCKET_NAME (Public ACL), \"\n    fi\n\n    # Check bucket policy\n    POLICY=$(aws s3api get-bucket-policy --bucket \"$BUCKET_NAME\" 2>/dev/null)\n    if echo \"$POLICY\" | grep -q \"Principal\\\": \\\"*\\\"\"; then\n        INSECURE_BUCKETS+=\"$BUCKET_NAME (Open Policy), \"\n    fi\ndone <<< \"$S3_BUCKETS\"\n\nif [ -z \"$INSECURE_BUCKETS\" ]; then\n    STATUS=\"success\"\n    OUTPUT=\"All S3 buckets have appropriate access controls\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"Insecure buckets detected: ${INSECURE_BUCKETS%??}\"\nfi\n\necho '{\"status\": \"'$STATUS'\", \"error\": \"\", \"name\": \"Protect Information through Access Control Lists\", \"output\": \"'$OUTPUT'\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\ncat \"$OUTPUT_FILE\"",
            "text": "Protect all information stored on systems with file system, network share,\nclaims, application, or database specific access control lists. These controls will\nenforce the principle that only authorized individuals should have access to the\ninformation based on their need to access the information as a part of their\nresponsibilities.\n\n\u25cf  \u25cf  \u25cf\n\nInternal Only - General\n\nPage 231",
            "title": "14.6_Protect_Information_through_Access_Control_Lists.md"
        }
    },
    "s3 buckets": {
        "benchmark-0": {
            "result": {
                "error": null,
                "name": "3.7 Ensure VPC flow logging is enabled in all VPCs",
                "output": "VPC Flow Logs are disabled for the following VPCs: vpc-0f8144ea4375c89ad",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"3.7 Ensure VPC flow logging is enabled in all VPCs\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"3.7 Ensure VPC flow logging is enabled in all VPCs\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Get all VPC IDs in current region\nVPC_IDS=$(aws ec2 describe-vpcs --query 'Vpcs[*].VpcId' --output text)\n\nFLOW_LOGS_DISABLED=()\n\nfor vpc_id in $VPC_IDS; do\n    # Check if flow logs exist for each VPC\n    FLOW_LOGS=$(aws ec2 describe-flow-logs --filter \"Name=resource-id,Values=$vpc_id\" --query 'FlowLogs[?FlowLogStatus==`ACTIVE`]' --output text)\n    \n    if [ -z \"$FLOW_LOGS\" ]; then\n        FLOW_LOGS_DISABLED+=(\"$vpc_id\")\n    fi\ndone\n\nif [ ${#FLOW_LOGS_DISABLED[@]} -eq 0 ]; then\n    STATUS=\"success\"\n    OUTPUT=\"VPC Flow Logs are enabled for all VPCs\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"VPC Flow Logs are disabled for the following VPCs: ${FLOW_LOGS_DISABLED[*]}\"\nfi\n\necho '{\"status\": \"'$STATUS'\", \"error\": \"\", \"name\": \"3.7 Ensure VPC flow logging is enabled in all VPCs\", \"output\": \"'$OUTPUT'\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\ncat \"$OUTPUT_FILE\"",
            "text": "Profile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic\ngoing to and from network interfaces in your VPC. After you've created a flow log, you\ncan view and retrieve its data in Amazon CloudWatch Logs. It is recommended that\nVPC Flow Logs be enabled for packet \"Rejects\" for VPCs.\n\nRationale:\n\nVPC Flow Logs provide visibility into network traffic that traverses the VPC and can be\nused to detect anomalous traffic or gain insights during security workflows.\n\nImpact:\n\nBy default, CloudWatch Logs will store logs indefinitely unless a specific retention\nperiod is defined for the log group. When choosing the number of days to retain, keep in\nmind that the average time it takes for an organization to realize they have been\nbreached is 210 days (at the time of this writing). Since additional time is required to\nresearch a breach, a minimum retention policy of 365 days allows for detection and\ninvestigation. You may also wish to archive the logs to a cheaper storage service rather\nthan simply deleting them. See the following AWS resource to manage CloudWatch\nLogs retention periods:\n\n1.  https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/Settin\n\ngLogRetention.html\n\nAudit:\n\nPerform the following to determine if VPC Flow logs are enabled:\nFrom Console:\n\n1.  Sign into the management console.\n2.  Select Services, then select VPC.\n3.  In the left navigation pane, select Your VPCs.\n4.  Select a VPC.\n5.  In the right pane, select the Flow Logs tab.\n6.  Ensure a Log Flow exists that has Active in the Status column.\n\nFrom Command Line:\n\n1.  Run the describe-vpcs command (OSX/Linux/UNIX) to list the VPC networks\n\navailable in the current AWS region:\n\nPage 135\n\nInternal Only - General\n\n\n```bash\naws ec2 describe-vpcs --region <region> --query Vpcs[].VpcId\n```\n\n\n2.  The command output returns the VpcId of VPCs available in the selected region.\n3.  Run the describe-flow-logs command (OSX/Linux/UNIX) using the VPC ID to\ndetermine if the selected virtual network has the Flow Logs feature enabled:\n\n\n```bash\naws ec2 describe-flow-logs --filter \"Name=resource-id,Values=<vpc-id>\"\n```\n\n\n\u2022\n\nIf there are no Flow Logs created for the selected VPC, the command output will\nreturn an empty list [].\n\n4.  Repeat step 3 for other VPCs in the same region.\n5.  Change the region by updating --region, and repeat steps 1-4 for each region.\n\nRemediation:\n\nPerform the following to enable VPC Flow Logs:\nFrom Console:\n\n1.  Sign into the management console.\n2.  Select Services, then select VPC.\n3.  In the left navigation pane, select Your VPCs.\n4.  Select a VPC.\n5.  In the right pane, select the Flow Logs tab.\n6.  If no Flow Log exists, click Create Flow Log.\n7.  For Filter, select Reject.\n8.  Enter a Role and Destination Log Group.\n9.  Click Create Log Flow.\n10. Click on CloudWatch Logs Group.\n\nNote: Setting the filter to \"Reject\" will dramatically reduce the accumulation of logging\ndata for this recommendation and provide sufficient information for the purposes of\nbreach detection, research, and remediation. However, during periods of least privilege\nsecurity group engineering, setting the filter to \"All\" can be very helpful in discovering\nexisting traffic flows required for the proper operation of an already running\nenvironment.\nFrom Command Line:\n\n1.  Create a policy document, name it role_policy_document.json, and paste the\n\nfollowing content:\n\nInternal Only - General\n\nPage 136\n\n\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"test\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"ec2.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n\n2.  Create another policy document, name it iam_policy.json, and paste the\n\nfollowing content:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\":[\n\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:DescribeLogGroups\",\n\"logs:DescribeLogStreams\",\n\"logs:PutLogEvents\",\n\"logs:GetLogEvents\",\n\"logs:FilterLogEvents\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n\n3.  Run the following command to create an IAM role:\n\n\n```bash\naws iam create-role --role-name <aws-support-iam-role> --assume-role-policy-\n```\n\ndocument file://<file-path>role_policy_document.json\n\n4.  Run the following command to create an IAM policy:\n\n\n```bash\naws iam create-policy --policy-name <iam-policy-name> --policy-document\n```\n\nfile://<file-path>iam-policy.json\n\nInternal Only - General\n\nPage 137\n\n\n\n\n\n5.  Run the attach-group-policy command, using the IAM policy ARN returned\n\nfrom the previous step to attach the policy to the IAM role:\n\n\n```bash\naws iam attach-group-policy --policy-arn arn:aws:iam::<aws-account-\n```\n\nid>:policy/<iam-policy-name> --group-name <group-name>\n\n\u2022\n\nIf the command succeeds, no output is returned.\n\n6.  Run the describe-vpcs command to get a list of VPCs in the selected region:\n\n\n```bash\naws ec2 describe-vpcs --region <region>\n```\n\n\n\u2022  The command output should return a list of VPCs in the selected region.\n\n7.  Run the create-flow-logs command to create a flow log for a VPC:\n\n\n```bash\naws ec2 create-flow-logs --resource-type VPC --resource-ids <vpc-id> --\n```\n\ntraffic-type REJECT --log-group-name <log-group-name> --deliver-logs-\npermission-arn <iam-role-arn>\n\n8.  Repeat step 7 for other VPCs in the selected region.\n9.  Change the region by updating --region, and repeat the remediation procedure\n\nfor each region.\n\nReferences:\n\n1.  CCE-79202-8\n2.  https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv8",
            "title": "3.7_Ensure_VPC_flow_logging_is_enabled_in_all_VPCs_(Automated).md"
        },
        "benchmark-1": {
            "result": {
                "error": "",
                "name": "2.1.1 Ensure S3 Bucket Policy is set to deny HTTP requests",
                "output": "Noncompliant buckets: testbucketsiol: No explicit HTTPS-only policy",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\n# Check AWS CLI configuration and credentials\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"2.1.1 Ensure S3 Bucket Policy is set to deny HTTP requests\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"2.1.1 Ensure S3 Bucket Policy is set to deny HTTP requests\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Get all S3 buckets\nBUCKETS=$(aws s3 ls | awk '{print $3}')\n\nNONCOMPLIANT_BUCKETS=()\n\n# Check each bucket's policy for secure transport\nfor BUCKET in $BUCKETS; do\n    # Try to get bucket policy\n    POLICY=$(aws s3api get-bucket-policy --bucket \"$BUCKET\" --query Policy --output text 2>/dev/null)\n    \n    if [ -z \"$POLICY\" ]; then\n        # No policy means default (both HTTP and HTTPS allowed)\n        NONCOMPLIANT_BUCKETS+=(\"$BUCKET: No policy (allowing both HTTP and HTTPS)\")\n        continue\n    fi\n\n    # Check for explicit HTTPS-only policy (deny HTTP)\n    if ! echo \"$POLICY\" | grep -q '\"aws:SecureTransport\": \"false\"' && \n       ! echo \"$POLICY\" | grep -q '\"s3:TlsVersion\": {\"NumericLessThan\": \"1.2\"}'; then\n        NONCOMPLIANT_BUCKETS+=(\"$BUCKET: No explicit HTTPS-only policy\")\n    fi\ndone\n\n# Determine compliance status\nif [ ${#NONCOMPLIANT_BUCKETS[@]} -eq 0 ]; then\n    STATUS=\"success\"\n    OUTPUT=\"All S3 buckets have HTTPS-only policies\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"Noncompliant buckets: ${NONCOMPLIANT_BUCKETS[*]}\"\nfi\n\n# Write results to output file\necho '{\n    \"status\": \"'$STATUS'\", \n    \"error\": \"\", \n    \"name\": \"2.1.1 Ensure S3 Bucket Policy is set to deny HTTP requests\", \n    \"output\": \"'$OUTPUT'\", \n    \"type\": \"automated\"\n}' > \"$OUTPUT_FILE\"\n\ncat \"$OUTPUT_FILE\"",
            "text": "(Automated)\n\nProfile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nAt the Amazon S3 bucket level, you can configure permissions through a bucket policy,\nmaking the objects accessible only through HTTPS.\n\nRationale:\n\nBy default, Amazon S3 allows both HTTP and HTTPS requests. To ensure that access\nto Amazon S3 objects is only permitted through HTTPS, you must explicitly deny HTTP\nrequests. Bucket policies that allow HTTPS requests without explicitly denying HTTP\nrequests will not comply with this recommendation.\n\nAudit:\n\nTo allow access to HTTPS, you can use a bucket policy with the effect allow and a\ncondition that checks for the key \"aws:SecureTransport\": \"true\". This means that\nHTTPS requests are allowed, but it does not deny HTTP requests. To explicitly deny\nHTTP access, ensure that there is also a bucket policy with the effect deny that contains\nthe key \"aws:SecureTransport\": \"false\". You may also require TLS by setting a\npolicy to deny any version lower than the one you wish to require, using the condition\nNumericLessThan and the key \"s3:TlsVersion\": \"1.2\".\nFrom Console:\n\n1.  Login to the AWS Management Console and open the Amazon S3 console using\n\nhttps://console.aws.amazon.com/s3/.\n2.  Select the check box next to the Bucket.\n3.  Click on 'Permissions', then click on Bucket Policy.\n4.  Ensure that a policy is listed that matches either:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\nInternal Only - General\n\nPage 76\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n<optional> and <bucket_name> will be specific to your account, and TLS version will\nbe site/policy specific to your organisation.\n\n5.  Repeat for all the buckets in your AWS account.\n\nFrom Command Line:\n\n1.  List all of the S3 Buckets\n\n\n```bash\naws s3 ls\n```\n\n\n2.  Using the list of buckets, run this command on each of them:\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> | grep aws:SecureTransport\n```\n\n\nor\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> | grep s3:TlsVersion\n```\n\n\nNOTE : If an error is thrown by the CLI, it means no policy has been configured for the\nspecified S3 bucket, and that by default it is allowing both HTTP and HTTPS requests.\n\n3.  Confirm that aws:SecureTransport is set to false (such as\n\naws:SecureTransport:false) or that s3:TlsVersion has a site-specific value.\n\n4.  Confirm that the policy line has Effect set to Deny 'Effect:Deny'\n\nRemediation:\n\nFrom Console:\n\nInternal Only - General\n\nPage 77\n\n\n\n\n1.  Login to the AWS Management Console and open the Amazon S3 console using\n\nhttps://console.aws.amazon.com/s3/.\n2.  Select the check box next to the Bucket.\n3.  Click on 'Permissions'.\n4.  Click 'Bucket Policy'.\n5.  Add either of the following to the existing policy, filling in the required information:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n6.  Save\n7.  Repeat for all the buckets in your AWS account that contain sensitive data.\n\nFrom Console\nUsing AWS Policy Generator:\n\n1.  Repeat steps 1-4 above.\n2.  Click on Policy Generator at the bottom of the Bucket Policy Editor.\n3.  Select Policy Type S3 Bucket Policy.\n4.  Add Statements:\n\n\u2022\n\nEffect = Deny\n\nInternal Only - General\n\nPage 78\n\n\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nPrincipal = *\n\nAWS Service = Amazon S3\n\nActions = *\n\nAmazon Resource Name = <ARN of the S3 Bucket>\n\n5.  Generate Policy.\n6.  Copy the text and add it to the Bucket Policy.\n\nFrom Command Line:\n\n1.  Export the bucket policy to a json file:\n\n\n```bash\naws s3api get-bucket-policy --bucket <bucket_name> --query Policy --output\n```\n\ntext > policy.json\n\n2.  Modify the policy.json file by adding either of the following:\n\n{\n\"Sid\": <optional>,\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": \"arn:aws:s3:::<bucket_name>/*\",\n\"Condition\": {\n\"Bool\": {\n\"aws:SecureTransport\": \"false\"\n}\n}\n}\n\nor\n\nInternal Only - General\n\nPage 79\n\n\n\n\n\n\n\n{\n\"Sid\": \"<optional>\",\n\"Effect\": \"Deny\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::<bucket_name>\",\n\"arn:aws:s3:::<bucket_name>/*\"\n],\n\"Condition\": {\n\"NumericLessThan\": {\n\"s3:TlsVersion\": \"1.2\"\n}\n}\n}\n\n3.  Apply this modified policy back to the S3 bucket:\n\n\n```bash\naws s3api put-bucket-policy --bucket <bucket_name> --policy\n```\n\nfile://policy.json\n\nDefault Value:\n\nBoth HTTP and HTTPS requests are allowed.\n\nReferences:\n\n1.  https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-\n\nconfig-rule/\n\n2.  https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-\n\ndefense-in-depth-to-help-secure-your-amazon-s3-data/\n\n3.  https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-\n\nbucket-policy.html\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv7",
            "title": "2.1.1_Ensure_S3_Bucket_Policy_is_set_to_deny_HTTP_requests.md"
        },
        "benchmark-2": {
            "result": {
                "name": "5.6 Ensure routing tables for VPC peering are least access",
                "output": "No VPC peering connections found",
                "status": "manual",
                "type": "manual"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"5.6 Ensure routing tables for VPC peering are least access\", \"output\": \"\", \"type\": \"manual\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"5.6 Ensure routing tables for VPC peering are least access\", \"output\": \"\", \"type\": \"manual\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Retrieve all VPC peering connections\nPEERING_CONNECTIONS=$(aws ec2 describe-vpc-peering-connections --query 'VpcPeeringConnections[*].VpcPeeringConnectionId' --output text)\n\nif [ -z \"$PEERING_CONNECTIONS\" ]; then\n    echo '{\"status\": \"manual\", \"name\": \"5.6 Ensure routing tables for VPC peering are least access\", \"output\": \"No VPC peering connections found\", \"type\": \"manual\"}' > \"$OUTPUT_FILE\"\n    exit 0\nfi\n\n# Initialize result variables\nDETAILED_OUTPUT=\"\"\nHAS_BROAD_ROUTES=false\n\n# Check routing tables for each peering connection\nfor PEERING_ID in $PEERING_CONNECTIONS; do\n    # Get VPC IDs for this peering connection\n    VPC_IDS=$(aws ec2 describe-vpc-peering-connections --vpc-peering-connection-ids \"$PEERING_ID\" --query 'VpcPeeringConnections[*].[RequesterVpcInfo.VpcId, AccepterVpcInfo.VpcId]' --output text)\n    \n    for VPC_ID in $VPC_IDS; do\n        ROUTE_TABLES=$(aws ec2 describe-route-tables --filters \"Name=vpc-id,Values=$VPC_ID\" --query 'RouteTables[*].RouteTableId' --output text)\n        \n        for ROUTE_TABLE_ID in $ROUTE_TABLES; do\n            PEERING_ROUTES=$(aws ec2 describe-route-tables --route-table-ids \"$ROUTE_TABLE_ID\" --query \"RouteTables[*].Routes[?GatewayId=='$PEERING_ID']\" --output text)\n            \n            if [ -n \"$PEERING_ROUTES\" ]; then\n                DESTINATION_CIDRS=$(aws ec2 describe-route-tables --route-table-ids \"$ROUTE_TABLE_ID\" --query \"RouteTables[*].Routes[?GatewayId=='$PEERING_ID'].DestinationCidrBlock\" --output text)\n                \n                for CIDR in $DESTINATION_CIDRS; do\n                    # Check if CIDR is too broad (e.g., 0.0.0.0/0 or very large subnets)\n                    if [[ \"$CIDR\" == \"0.0.0.0/0\" || $(echo \"$CIDR\" | cut -d'/' -f2) -lt 24 ]]; then\n                        HAS_BROAD_ROUTES=true\n                        DETAILED_OUTPUT+=\"Broad route found: Route Table $ROUTE_TABLE_ID in VPC $VPC_ID has peering route to $CIDR. \"\n                    fi\n                done\n            fi\n        done\n    done\ndone\n\nif [ \"$HAS_BROAD_ROUTES\" = true ]; then\n    echo '{\"status\": \"fail\", \"error\": \"Broad VPC peering routes detected\", \"name\": \"5.6 Ensure routing tables for VPC peering are least access\", \"output\": \"'\"$DETAILED_OUTPUT\"'\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\nelse\n    echo '{\"status\": \"success\", \"error\": \"\", \"name\": \"5.6 Ensure routing tables for VPC peering",
            "text": "(Manual)\n\nProfile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nOnce a VPC peering connection is established, routing tables must be updated to\nenable any connections between the peered VPCs. These routes can be as specific as\ndesired, even allowing for the peering of a VPC to only a single host on the other side of\nthe connection.\n\nRationale:\n\nBeing highly selective in peering routing tables is a very effective way to minimize the\nimpact of a breach, as resources outside of these routes are inaccessible to the peered\nVPC.\n\nAudit:\n\nReview the routing tables of peered VPCs to determine whether they route all subnets\nof each VPC and whether this is necessary to accomplish the intended purposes of\npeering the VPCs.\nFrom Command Line:\n\n1.  List all the route tables from a VPC and check if the \"GatewayId\" is pointing to a\n\n<peering-connection-id> (e.g., pcx-1a2b3c4d) and if the\n\"DestinationCidrBlock\" is as specific as desired:\n\n\n```bash\naws ec2 describe-route-tables --filter \"Name=vpc-id,Values=<vpc-id>\" --query\n```\n\n\"RouteTables[*].{RouteTableId:RouteTableId, VpcId:VpcId, Routes:Routes,\nAssociatedSubnets:Associations[*].SubnetId}\"\n\nRemediation:\n\nRemove and add route table entries to ensure that the least number of subnets or hosts\nrequired to accomplish the purpose of peering are routable.\nFrom Command Line:\n\n1.  For each <route-table-id> that contains routes that are non-compliant with\n\nyour routing policy (granting more access than desired), delete the non-compliant\nroute:\n\n\n```bash\naws ec2 delete-route --route-table-id <route-table-id> --destination-cidr-\n```\n\nblock <non-compliant-destination-cidr>\n\nInternal Only - General\n\nPage 232\n\n\n2.  Create a new compliant route:\n\n\n```bash\naws ec2 create-route --route-table-id <route-table-id> --destination-cidr-\n```\n\nblock <compliant-destination-cidr> --vpc-peering-connection-id <peering-\nconnection-id>\n\nReferences:\n\n1.  https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/peering-\n\nconfigurations-partial-access.html\n\n2.  https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/create-\n\nvpc-peering-connection.html\n\nAdditional Information:\n\nIf an organization has an AWS Transit Gateway implemented in its VPC architecture, it\nshould look to apply the recommendation above for a \"least access\" routing architecture\nat the AWS Transit Gateway level, in combination with what must be implemented at\nthe standard VPC route table. More specifically, to route traffic between two or more\nVPCs via a Transit Gateway, VPCs must have an attachment to a Transit Gateway\nroute table as well as a route. Therefore, to avoid routing traffic between VPCs, an\nattachment to the Transit Gateway route table should only be added where there is an\nintention to route traffic between the VPCs. As Transit Gateways are capable of hosting\nmultiple route tables, it is possible to group VPCs by attaching them to a common route\ntable.\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv7",
            "title": "5.6_Ensure_routing_tables_for_VPC_peering_are_\"least_access\".md"
        },
        "benchmark-3": {
            "result": {
                "error": "No multi-region CloudTrail trail found",
                "name": "4.13 Ensure route table changes are monitored",
                "output": "No multi-region trail configured",
                "status": "fail",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\n# Check AWS CLI configuration\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Function to check CloudTrail configuration\ncheck_cloudtrail_config() {\n    # Get multi-region CloudTrail trails\n    TRAILS=$(aws cloudtrail describe-trails --query 'trailList[?IsMultiRegionTrail==`true`]' --output json)\n    \n    if [ -z \"$TRAILS\" ] || [ \"$TRAILS\" == \"[]\" ]; then\n        echo '{\"status\": \"fail\", \"error\": \"No multi-region CloudTrail trail found\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"No multi-region trail configured\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n        exit 1\n    fi\n\n    # Iterate through trails\n    for trail in $(echo \"$TRAILS\" | grep -E '\"Name\"|\"CloudWatchLogsLogGroupArn\"' | cut -d':' -f2 | tr -d ' \",'); do\n        # Check trail status\n        TRAIL_STATUS=$(aws cloudtrail get-trail-status --name \"$trail\" --query 'IsLogging' --output text)\n        \n        if [ \"$TRAIL_STATUS\" != \"true\" ]; then\n            echo '{\"status\": \"fail\", \"error\": \"CloudTrail trail not logging\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"Trail '\"$trail\"' is not logging\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n            exit 1\n        fi\n\n        # Get log group name\n        LOG_GROUP=$(aws cloudtrail describe-trails --trail-name-list \"$trail\" --query 'trailList[0].CloudWatchLogsLogGroupArn' --output text | cut -d':' -f7)\n\n        # Check metric filters for route table changes\n        METRIC_FILTERS=$(aws logs describe-metric-filters --log-group-name \"$LOG_GROUP\" --query 'metricFilters[?filterPattern && contains(filterPattern, \"CreateRoute\") && contains(filterPattern, \"CreateRouteTable\") && contains(filterPattern, \"ReplaceRoute\") && contains(filterPattern, \"DeleteRoute\")]' --output json)\n\n        if [ -z \"$METRIC_FILTERS\" ] || [ \"$METRIC_FILTERS\" == \"[]\" ]; then\n            echo '{\"status\": \"fail\", \"error\": \"No metric filter for route table changes\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"No metric filter configured for route table changes\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n            exit 1\n        fi\n\n        # Get metric name\n        METRIC_NAME=$(echo \"$METRIC_FILTERS\" | grep -o '\"metricName\": *\"[^\"]*\"' | cut -d'\"' -f4)\n\n        # Check CloudWatch alarms\n        ALARMS=$(aws cloudwatch describe-alarms --query \"MetricAlarms[?MetricName=='$METRIC_NAME']\" --output json)\n\n        if [ -z \"$ALARMS\" ] || [ \"$ALARMS\" == \"[]\" ]; then\n            echo '{\"status\": \"fail\", \"error\": \"No CloudWatch alarm for route table changes\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"No alarm configured for route table changes\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n            exit 1\n        fi\n\n        # Success scenario\n        echo '{\"status\": \"success\", \"error\": \"\", \"name\": \"4.13 Ensure route table changes are monitored\", \"output\": \"Route table changes monitoring configured correctly\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n        exit 0\n    done\n}\n\n# Execute configuration check\ncheck_cloudtrail_config\n\n# If no trails found or checks",
            "text": "Profile Applicability:\n\n\u2022  Level 1\n\nDescription:\n\nReal-time monitoring of API calls can be achieved by directing CloudTrail Logs to\nCloudWatch Logs or an external Security Information and Event Management (SIEM)\nenvironment, and establishing corresponding metric filters and alarms.\n\nRouting tables are used to route network traffic between subnets and to network\ngateways.\n\nIt is recommended that a metric filter and alarm be established for changes to route\ntables.\n\nRationale:\n\nCloudWatch is an AWS native service that allows you to observe and monitor resources\nand applications. CloudTrail logs can also be sent to an external Security Information\nand Event Management (SIEM) environment for monitoring and alerting.\n\nMonitoring changes to route tables will help ensure that all VPC traffic flows through the\nexpected path and prevent any accidental or intentional modifications that may lead to\nuncontrolled network traffic. An alarm should be triggered every time an AWS API call is\nperformed to create, replace, delete, or disassociate a route table.\n\nAudit:\n\nIf you are using CloudTrail trails and CloudWatch, perform the following to ensure that\nthere is at least one active multi-region CloudTrail trail with the prescribed metric filters\nand alarms configured:\n\n1.  Identify the log group name that is configured for use with the active multi-region\n\nCloudTrail trail:\n\n\u2022  List all CloudTrail trails: aws cloudtrail describe-trails\n\u2022\n\nIdentify multi-region CloudTrail trails: Trails with \"IsMultiRegionTrail\"\nset to true\n\n\u2022  Note the value associated with \"Name\":<trail-name>\n\u2022  Note the <trail-log-group-name> within the value associated with\n\n\"CloudWatchLogsLogGroupArn\"\n\no  Example: arn:aws:logs:<region>:<account-id>:log-\n\ngroup:<trail-log-group-name>:*\n\n\u2022  Ensure the identified multi-region CloudTrail trail is active:\n\no  aws cloudtrail get-trail-status --name <trail-name>\n\n\u25aa  Ensure IsLogging is set to TRUE\n\nPage 200\n\nInternal Only - General\n\n\u2022  Ensure the identified multi-region CloudTrail trail captures all management\n\nevents:\n\no  aws cloudtrail get-event-selectors --trail-name <trail-name>\n\u25aa  Ensure there is at least one event selector for a trail with\n\nIncludeManagementEvents set to true and ReadWriteType set to\nAll\n\n2.  Get a list of all associated metric filters for the <trail-log-group-name>\n\ncaptured in step 1:\n\n\n```bash\naws logs describe-metric-filters --log-group-name <trail-log-group-\n```\n\nname>\n\n3.  Ensure the output from the above command contains the following:\n\n\"filterPattern\": \"{($.eventSource = ec2.amazonaws.com) && ($.eventName\n= CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName =\nReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) ||\n($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) ||\n($.eventName = DisassociateRouteTable) }\"\n\n4.  Note the <route-table-changes-metric> value associated with the\n\nfilterPattern from step 3.\n\n5.  Get a list of CloudWatch alarms, and filter on the <route-table-changes-\n\nmetric> captured in step 4:\n\n\n```bash\naws cloudwatch describe-alarms --query\n```\n\n'MetricAlarms[?MetricName==<route-table-changes-metric>]'\n\n6.  Note the AlarmActions value; this will provide the SNS topic ARN value.\n7.  Ensure there is at least one active subscriber to the SNS topic:\n\n\n```bash\naws sns list-subscriptions-by-topic --topic-arn <sns-topic-arn>\n```\n\n\n\u2022  At least one subscription should have \"SubscriptionArn\" with a valid AWS ARN.\no  Example of valid \"SubscriptionArn\": arn:aws:sns:<region>:<account-\n\nid>:<sns-topic-name>:<subscription-id>\n\nRemediation:\n\nIf you are using CloudTrail trails and CloudWatch, perform the following steps to set up\nthe metric filter, alarm, SNS topic, and subscription:\n\n1.  Create a metric filter based on the provided filter pattern that checks for route\n\ntable changes and uses the <trail-log-group-name> taken from audit step 1:\n\nInternal Only - General\n\nPage 201\n\n\n\n```bash\naws logs put-metric-filter --log-group-name <trail-log-group-name> --\n```\n\nfilter-pattern '{ ($.eventName = CreateRoute) || ($.eventName =\nCreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName =\nReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) ||\n($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable)\n}'\n\nNote: You can choose your own metricName and metricNamespace strings.\nUsing the same metricNamespace for all Foundations Benchmark metrics will\ngroup them together.\n\n2.  Create an SNS topic that the alarm will notify:\n\n\n```bash\naws sns create-topic --name <sns-topic-name>\n```\n\n\nNote: You can execute this command once and then reuse the same topic for all\nmonitoring alarms.\nNote: Capture the TopicArn that is displayed when creating the SNS topic in\nstep 2.\n\n3.  Create an SNS subscription for the topic created in step 2:\n\n\n```bash\naws sns subscribe --topic-arn <sns-topic-arn> --protocol <sns-protocol>\n```\n\n--notification-endpoint <sns-subscription-endpoints>\n\nNote: You can execute this command once and then reuse the same\nsubscription for all monitoring alarms.\n\n4.  Create an alarm that is associated with the CloudWatch Logs metric filter created\n\nin step 1 and the SNS topic created in step 2:\n\n\n```bash\naws cloudwatch put-metric-alarm --alarm-name <route-table-changes-\n```\n\nalarm> --metric-name <route-table-changes-metric> --statistic Sum --\nperiod 300 --threshold 1 --comparison-operator\nGreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace\n'CISBenchmark' --alarm-actions <sns-topic-arn>\n\nReferences:\n\n1.  CCE-79198-8\n2.  https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-\n\nlog-files-from-multiple-regions.html\n\n3.  https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-\n\nfor-cloudtrail.html\n\n4.  https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html\n\nInternal Only - General\n\nPage 202\n\nAdditional Information:\n\nConfiguring a log metric filter and alarm on a multi-region (global) CloudTrail trail:\n\n\u2022  ensures that activities from all regions (both used and unused) are monitored\n\u2022  ensures that activities on all supported global services are monitored\n\u2022  ensures that all management events across all regions are monitored\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv8\n\nv7\n\nv7\n\nv7",
            "title": "4.13_Ensure_route_table_changes_are_monitored_(Manual).md"
        },
        "benchmark-4": {
            "result": {
                "error": "",
                "name": "Ensure IAM users are managed centrally via identity federation or AWS Organizations",
                "output": "All accounts are compliant. No local IAM users found in non-master accounts.",
                "status": "success",
                "type": "automated"
            },
            "script": "#!/bin/bash\nOUTPUT_FILE=\"aws_benchmark_test.json\"\n\n# Check for AWS CLI configuration\nif [[ -z \"$AWS_ACCESS_KEY_ID\" || -z \"$AWS_SECRET_ACCESS_KEY\" || -z \"$AWS_DEFAULT_REGION\" ]]; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS environment variables not set\", \"name\": \"Ensure IAM users are managed centrally via identity federation or AWS Organizations\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Check if AWS CLI is configured correctly\nif ! aws sts get-caller-identity >/dev/null 2>&1; then\n    echo '{\"status\": \"fail\", \"error\": \"AWS CLI not configured\", \"name\": \"Ensure IAM users are managed centrally via identity federation or AWS Organizations\", \"output\": \"\", \"type\": \"automated\"}' > \"$OUTPUT_FILE\"\n    exit 1\nfi\n\n# Initialize variables\nTOTAL_ACCOUNTS=0\nNON_COMPLIANT_ACCOUNTS=0\nNON_COMPLIANT_ACCOUNT_LIST=\"\"\n\n# Retrieve list of accounts in AWS Organizations\nACCOUNTS=$(aws organizations list-accounts --query 'Accounts[*].Id' --output text)\n\n# Check each account for local IAM users\nfor ACCOUNT_ID in $ACCOUNTS; do\n    # Skip if it's the master account\n    MASTER_ACCOUNT=$(aws organizations describe-organization --query 'Organization.MasterAccountId' --output text)\n    if [[ \"$ACCOUNT_ID\" == \"$MASTER_ACCOUNT\" ]]; then\n        continue\n    fi\n\n    # Try to switch role and list IAM users\n    USERS=$(aws iam list-users --query 'Users[?not_contains(UserName, \"service\")].UserName' --output text 2>/dev/null)\n    \n    TOTAL_ACCOUNTS=$((TOTAL_ACCOUNTS + 1))\n    \n    if [[ -n \"$USERS\" ]]; then\n        NON_COMPLIANT_ACCOUNTS=$((NON_COMPLIANT_ACCOUNTS + 1))\n        NON_COMPLIANT_ACCOUNT_LIST+=\"$ACCOUNT_ID \"\n    fi\ndone\n\n# Determine compliance status\nif [[ $NON_COMPLIANT_ACCOUNTS -eq 0 ]]; then\n    STATUS=\"success\"\n    OUTPUT=\"All accounts are compliant. No local IAM users found in non-master accounts.\"\nelse\n    STATUS=\"fail\"\n    OUTPUT=\"$NON_COMPLIANT_ACCOUNTS out of $TOTAL_ACCOUNTS accounts have local IAM users. Non-compliant account IDs: $NON_COMPLIANT_ACCOUNT_LIST\"\nfi\n\n# Output results\necho '{\n    \"status\": \"'$STATUS'\",\n    \"error\": \"\",\n    \"name\": \"Ensure IAM users are managed centrally via identity federation or AWS Organizations\",\n    \"output\": \"'$OUTPUT'\",\n    \"type\": \"automated\"\n}' > \"$OUTPUT_FILE\"\ncat \"$OUTPUT_FILE\"",
            "text": "federation or AWS Organizations for multi-account environments\n(Manual)\n\nProfile Applicability:\n\n\u2022  Level 2\n\nDescription:\n\nIn multi-account environments, IAM user centralization facilitates greater user control.\nUser access beyond the initial account is then provided via role assumption.\nCentralization of users can be accomplished through federation with an external identity\nprovider or through the use of AWS Organizations.\n\nRationale:\n\nCentralizing IAM user management to a single identity store reduces complexity and\nthus the likelihood of access management errors.\n\nAudit:\n\nFor multi-account AWS environments with an external identity provider:\n\n1.  Determine the master account for identity federation or IAM user management\n2.  Login to that account through the AWS Management Console\n3.  Click Services\n4.  Click IAM\n5.  Click Identity providers\n6.  Verify the configuration\n\nFor multi-account AWS environments with an external identity provider, as well as for\nthose implementing AWS Organizations without an external identity provider:\n\n1.  Determine all accounts that should not have local users present\n2.  Log into the AWS Management Console\n3.  Switch role into each identified account\n4.  Click Services\n5.  Click IAM\n6.  Click Users\n7.  Confirm that no IAM users representing individuals are present\n\nInternal Only - General\n\nPage 71\n\nRemediation:\n\nThe remediation procedure will vary based on each individual organization's\nimplementation of identity federation and/or AWS Organizations, with the acceptance\ncriteria that no non-service IAM users and non-root accounts are present outside the\naccount providing centralized IAM user management.\n\nCIS Controls:\n\nControls\nVersion\n\nControl\n\nIG 1  IG 2  IG 3\n\nv8\n\nv7",
            "title": "1.21_Ensure_IAM_users_are_managed_centrally_via_identity.md"
        }
    }
}